{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9043e3c",
   "metadata": {},
   "source": [
    "# BERT Sarcasm Analysis\n",
    "\n",
    "https://medium.com/@nguyenduchuyvn/sarcasm-detection-with-machine-learning-92538da893ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdc0060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\witch\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchinfo\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "import sys\n",
    "from IPython.core import ultratb\n",
    "sys.excepthook = ultratb.FormattedTB(color_scheme='Linux', call_pdb=False)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6250272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "\n",
    "        self.labels = np.array(Y)\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', max_length = 512,\n",
    "                                # truncation=True,\n",
    "                                return_tensors=\"pt\") for text in X]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157900cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X,Y, learning_rate, epochs, batch_size):\n",
    "\n",
    "    train = Dataset(X,Y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "        train_label = train_label.to(device)\n",
    "        mask = train_input['attention_mask'].to(device)\n",
    "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask)\n",
    "      \n",
    "        batch_loss = criterion(output, train_label)\n",
    "        total_loss_train += batch_loss.item()\n",
    "      \n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        total_acc_train += acc\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(X): .3f} | Train Accuracy: {total_acc_train / len(X): .3f}')\n",
    "\n",
    "\n",
    "def evaluate(model, X,Y, batch_size):\n",
    "\n",
    "    test = Dataset(X, Y)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size,shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            y_pred.append(output.argmax(dim=1))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a71ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "\n",
    "X = df['headline']\n",
    "Y = df['is_sarcastic']\n",
    "\n",
    "data_split = int(df.shape[0] * 0.75)\n",
    "X_train, X_test = X[:data_split], X[data_split:]\n",
    "y_train, y_test = Y[:data_split], Y[data_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c843f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619 headlines\n",
      "14985 serious headlines.\n",
      "13634 sarcastic headlines.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>1</td>\n",
       "      <td>jews to celebrate rosh hashasha or something</td>\n",
       "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>1</td>\n",
       "      <td>internal affairs investigator disappointed con...</td>\n",
       "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>0</td>\n",
       "      <td>the most beautiful acceptance speech this week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>1</td>\n",
       "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
       "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28618</th>\n",
       "      <td>1</td>\n",
       "      <td>dad clarifies this not a food stop</td>\n",
       "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28619 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  thirtysomething scientists unveil doomsday clo...   \n",
       "1                 0  dem rep. totally nails why congress is falling...   \n",
       "2                 0  eat your veggies: 9 deliciously different recipes   \n",
       "3                 1  inclement weather prevents liar from getting t...   \n",
       "4                 1  mother comes pretty close to using word 'strea...   \n",
       "...             ...                                                ...   \n",
       "28614             1       jews to celebrate rosh hashasha or something   \n",
       "28615             1  internal affairs investigator disappointed con...   \n",
       "28616             0  the most beautiful acceptance speech this week...   \n",
       "28617             1  mars probe destroyed by orbiting spielberg-gat...   \n",
       "28618             1                 dad clarifies this not a food stop   \n",
       "\n",
       "                                            article_link  \n",
       "0      https://www.theonion.com/thirtysomething-scien...  \n",
       "1      https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2      https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3      https://local.theonion.com/inclement-weather-p...  \n",
       "4      https://www.theonion.com/mother-comes-pretty-c...  \n",
       "...                                                  ...  \n",
       "28614  https://www.theonion.com/jews-to-celebrate-ros...  \n",
       "28615  https://local.theonion.com/internal-affairs-in...  \n",
       "28616  https://www.huffingtonpost.com/entry/andrew-ah...  \n",
       "28617  https://www.theonion.com/mars-probe-destroyed-...  \n",
       "28618  https://www.theonion.com/dad-clarifies-this-no...  \n",
       "\n",
       "[28619 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape[0], 'headlines')\n",
    "print(df.loc[df['is_sarcastic'] == 0].shape[0], 'serious headlines.')\n",
    "print(df.loc[df['is_sarcastic'] == 1].shape[0], 'sarcastic headlines.')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98e412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertClassifier                                          --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              3,906,816\n",
       "│    │    └─Embedding: 3-2                              65,536\n",
       "│    │    └─Embedding: 3-3                              256\n",
       "│    │    └─LayerNorm: 3-4                              256\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             396,544\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 16,512\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           258\n",
       "├─ReLU: 1-4                                             --\n",
       "================================================================================\n",
       "Total params: 4,386,178\n",
       "Trainable params: 4,386,178\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "batch_size = 32\n",
    "model = BertClassifier()\n",
    "LR = 1e-4\n",
    "\n",
    "torchinfo.summary(model, dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd72aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 671/671 [10:31<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.013 | Train Accuracy:  0.803\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train, y_train, LR, EPOCHS, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e72d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 224/224 [02:20<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = evaluate(model, X_test,y_test, batch_size)\n",
    "\n",
    "y_pred_ = torch.cat(y_pred, dim=0)\n",
    "\n",
    "y_pred_ = y_pred_.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9851fcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      3745\n",
      "           1       0.91      0.79      0.85      3410\n",
      "\n",
      "    accuracy                           0.86      7155\n",
      "   macro avg       0.87      0.86      0.86      7155\n",
      "weighted avg       0.87      0.86      0.86      7155\n",
      "\n",
      "0.8596055738051518\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.values, y_pred_))\n",
    "print(roc_auc_score(y_test, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb04a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'sarcasm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7110ba",
   "metadata": {},
   "source": [
    "## Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e776072d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertClassifier                                          --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              3,906,816\n",
       "│    │    └─Embedding: 3-2                              65,536\n",
       "│    │    └─Embedding: 3-3                              256\n",
       "│    │    └─LayerNorm: 3-4                              256\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             396,544\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 16,512\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           258\n",
       "├─ReLU: 1-4                                             --\n",
       "================================================================================\n",
       "Total params: 4,386,178\n",
       "Trainable params: 4,386,178\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"sarcasm.model\"))\n",
    "\n",
    "torchinfo.summary(model, dtypes=['torch.IntTensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a73f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask\n",
      "\tmask.shape = torch.Size([1, 512])\n",
      "\tmask[0][:20] = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "input_id\n",
      "\tinput_id.shape = torch.Size([1, 512])\n",
      "\tinput_id[0][:20] = tensor([  101,  3076,  2342,  2053,  2062,  2084,  1017,  2847,  1997,  3637,\n",
      "         1010, 21318, 14194,  6950,  6592,   102,     0,     0,     0,     0])\n",
      "output = tensor([[0.2917, 2.5287]])\n",
      "label = YES\n",
      "score = 0.90352714\n"
     ]
    }
   ],
   "source": [
    "def single_batch_predict(x):\n",
    "    with torch.no_grad():\n",
    "        texts = tokenizer(x, padding='max_length', max_length = 512, return_tensors=\"pt\")\n",
    "        \n",
    "        mask = texts['attention_mask'].to(device)\n",
    "        input_id = texts['input_ids'].squeeze(1).to(device) # squueze remove all dimensions size 1\n",
    "        print('mask')\n",
    "        print('\\tmask.shape =', mask.shape)\n",
    "        print('\\tmask[0][:20] =', mask[0][:20])\n",
    "        print('input_id')\n",
    "        print('\\tinput_id.shape =', input_id.shape)\n",
    "        print('\\tinput_id[0][:20] =', input_id[0][:20])\n",
    "        \n",
    "        output = model(input_id, mask)\n",
    "        print('output =', output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "labels = ['NO', 'YES']\n",
    "prediction = single_batch_predict(\"student need no more than 3 hours of sleep, UIUC researchers suggest\")\n",
    "prediction = softmax(prediction[0])\n",
    "print('label =', labels[prediction.argmax()])\n",
    "print('score =', prediction[prediction.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f84acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_batch_train(x, y):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    texts = tokenizer(x, padding='max_length', max_length = 512, return_tensors=\"pt\")\n",
    "    mask = texts['attention_mask'].to(device)\n",
    "    input_id = texts['input_ids'].squeeze(1).to(device) # squueze remove all dimensions size 1\n",
    "    \n",
    "    output = model(input_id, mask)\n",
    "\n",
    "    batch_loss = criterion(output, torch.tensor(y))\n",
    "    \n",
    "    model.zero_grad()\n",
    "    batch_loss.backward() # calculate gradients and store then in the model's tensors\n",
    "    optimizer.step() # optimize model parameters using the stored gradients\n",
    "\n",
    "# single_batch_train(['employee returns from vacation refreshed, ready to waste time'], [0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
